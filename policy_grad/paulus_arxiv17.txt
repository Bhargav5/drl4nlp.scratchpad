--------------------------------------------------------------------------------------------------------------------------------
11. A Deep Reinforced Model for Abstractive Summarization
Romain Paulus, Caiming Xiong, Richard Socher
Arxiv, 2017
--------------------------------------------------------------------------------------------------------------------------------

Goal & Challenges:
* AS for longer documents
Challenges:
* Repetitive - repeating phrase
* Incoherent phrases
* Exposure bias - assume ground truth is provided at each step during training
* Number of potential summaries increases with the sequence size (bleu mismatch)

Novelty/Contributions:
Introduce 2 things:
* Use intra-temporal attention in encoder that records previous attention weights for each of the input tokens while a sequential intra-attention model in the decoder takes into account which words have already been generated by the decoder.
* Propose a new objective function by combining the MLE Cross-Entropy with rewards from PG RL to reduce exposure bias.
* Evalute model on CNN/Daily Mail, New York Times dataset
* 1st model for AS on NYT dataset

RL: 
* Self-critical PG algo.
* Baseline: Greedy Search
* Reward: our choice (bleu or rouge)
* Hybrid objective: MLE (for readability) + RL

Pros/Cons:
* Combines supervised word prediction and RL to make readable summaries
* Makes no assumpation about type of decoder RNN
* SOTA in CNN/Daily Mail
* ROUGE has shortcomings

Future directions:
* Using the intra-attention decoder and combined training objective for other Seq2Seq tasks.
